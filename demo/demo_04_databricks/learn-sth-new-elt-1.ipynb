{"cells":[{"cell_type":"markdown","source":["#Notebook Description\n**Author**: Slawomir Drzymala\n\n**Description:**   \nThis notebook is getting the data from the raw layer of the data lake and preparing a dataset that can be used in further analysis placing them in the curated layer"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"669e144b-fc7c-41e5-99ed-190dbbec49bb"}}},{"cell_type":"markdown","source":["#Set up connection to data lake on Azure\n\n**Things to be noticed:**   \n* **sensitive data alert** - please note that this is not recommended to store any key or any other sensitve data in the notebooks, this is just to make the code more simple for the demo. For real work please use Azure KeyVault or databricks secrets.\n* **multiple ways to connect to Azure data lake** - there are multiple options to connect to the Azure data lake, we can use the access key or the service principal, we can also mount the storage account so the storage account will be visible in many notebooks, please see link below for mode details"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68048e27-03b7-4a38-87be-8d08e401ba82"}}},{"cell_type":"code","source":["#vide https://docs.databricks.com/_static/notebooks/data-import/azure-data-lake-store.html\n#vide https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-get-started\nspark.conf.set(\n  \"fs.azure.account.key.sdsalearnsthnew.dfs.core.windows.net\", \n  \"RJMELuc9ffZPf5D0gwcbxJp+hWTkQuW8lmWa1DRFSF59aDiatDsMJ6X/yC/dHZtB7kdGl3cJIrYry++6EnCb5g==\" \n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fc67378-c5fb-4c69-af11-6c01463e2c53"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Check connection to Azure data lake and list files in folder\n\n**Things to be noticed:**   \n* **display** - display is the magic Databricks function that can be used for visualization of many different objects including spark or pandas dataframes\n* **dbutils.fs.ls** - display is the magic Databricks function that can be used to list the files in the local environment or the connected storage acccounts, here the Azure data lake"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18cd39ee-e9a7-4b3a-9c78-421ffdbdeba3"}}},{"cell_type":"code","source":["file_path = \"abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/\"\n\ndisplay(\n  dbutils.fs.ls(file_path)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adeb2757-149c-4947-8b8f-78af377deabb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2010.json","ZET_2010.json",12440846],["abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2011.json","ZET_2011.json",12472076],["abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2012.json","ZET_2012.json",11404607],["abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2013.json","ZET_2013.json",12488122],["abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2014.json","ZET_2014.json",11862115],["abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2015.json","ZET_2015.json",12890685],["abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2016.json","ZET_2016.json",13407331],["abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2017.json","ZET_2017.json",12803766],["abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2018.json","ZET_2018.json",13939163],["abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2019.json","ZET_2019.json",14007212],["abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2020.json","ZET_2020.json",13372711]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2010.json</td><td>ZET_2010.json</td><td>12440846</td></tr><tr><td>abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2011.json</td><td>ZET_2011.json</td><td>12472076</td></tr><tr><td>abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2012.json</td><td>ZET_2012.json</td><td>11404607</td></tr><tr><td>abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2013.json</td><td>ZET_2013.json</td><td>12488122</td></tr><tr><td>abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2014.json</td><td>ZET_2014.json</td><td>11862115</td></tr><tr><td>abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2015.json</td><td>ZET_2015.json</td><td>12890685</td></tr><tr><td>abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2016.json</td><td>ZET_2016.json</td><td>13407331</td></tr><tr><td>abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2017.json</td><td>ZET_2017.json</td><td>12803766</td></tr><tr><td>abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2018.json</td><td>ZET_2018.json</td><td>13939163</td></tr><tr><td>abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2019.json</td><td>ZET_2019.json</td><td>14007212</td></tr><tr><td>abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/zet/ZET_2020.json</td><td>ZET_2020.json</td><td>13372711</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Read files from Azure data lake\n\n**Things to be noticed:**   \n* **data_schema** - Databricks will try to retrive the schema from the file itself, but we can also specify the custom data schema for the files that we want to load, if we specify the schema we will make sure that all of the data will be handled properly and also if we won't specify a particular column in a schema then even if the column will be present in the file it will be ignored during the data load\n* **wildcards in the path** - please note that we can use a wildcards in any part of the path and that will allow us to load many files or folders according to the provided path template\n* **.json()** - we are going to read the json files so we will use the json function, but for the other file types we would need to use a different functions\n* **encoding** - encoding will make sure that all of the unicode special characters will be discovered correctly\n* **spark dataframe vs pandas dataframe** - please note that the dataframe that will be created it's not the pandas data frame, but the spark data frame, please check the differences here: https://towardsdatascience.com/parallelize-pandas-dataframe-computations-w-spark-dataframe-bba4c924487c or https://medium.com/@chris_bour/6-differences-between-pandas-and-spark-dataframes-1380cec394d2 here."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c551684-1eec-4808-86f9-bb13c6d58edb"}}},{"cell_type":"code","source":["from pyspark.sql.types import TimestampType, StringType\n\ndata_schema = [\n                StructField('datetime', TimestampType(), True), \n                StructField('artist', StringType(), True),\n                StructField('title', StringType(), True)\n ]\nfinal_struc = StructType(fields=data_schema)\n\n# read all files from all radio stations\nfile_path = \"abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/raw-initial/*/*.json\"\ndf_playlist = spark.read.option('encoding', 'UTF-8').json(file_path, multiLine=True, schema=final_struc)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"250520d9-c5eb-4975-8c07-73ba4bf04ccc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Check schema of the created dataframe\n\n**Things to be noticed:**   \n* **df.printSchema()** - is the function of the dataframe that can be used to display the schema - columns and types - of the dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6e53d08-86bd-4cf5-a8d1-09bc9e99ef91"}}},{"cell_type":"code","source":["df_playlist.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93fc0be2-c30c-4039-8a03-dfdd1d24683e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- datetime: timestamp (nullable = true)\n |-- artist: string (nullable = true)\n |-- title: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- datetime: timestamp (nullable = true)\n-- artist: string (nullable = true)\n-- title: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Check number of rows\n\n**Things to be noticed:**   \n* **df.count()** - is the function of the dataframe that we can use to get the number of rows from the dataframe, please not that the Pandas len() - as an exmaple - won't work here"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15a06887-a11c-4f43-b3d4-0c384ed605ea"}}},{"cell_type":"code","source":["df_playlist.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c4e1128-1278-4798-b611-a9bbee62ee83"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[147]: 4614665</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[147]: 4614665</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Display 5 sample rows\n\n**Things to be noticed:**   \n* **display** - display is the magic Databricks function that can be used for visualization of many different objects including spark or pandas dataframes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"084046d7-d85d-49bf-b993-23aca823b073"}}},{"cell_type":"code","source":["display(\n  df_playlist.head(5)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c64d9050-50f7-45f5-852f-9e4064c18766"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["2019-12-31T00:00:00.000+0000","Peja/slums Attack","Szacunek Ludzi Ulicy (Explicit)"],["2019-12-31T00:05:00.000+0000","Tymek/tede","Rainman (Explicit)"],["2019-12-31T00:08:00.000+0000","Taconafide","Metallica 808 (Explicit)"],["2019-12-31T00:12:00.000+0000","Nautilus","Blat"],["2019-12-31T00:16:00.000+0000","Young Multi","Jeden Dzien (Explicit)"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"datetime","type":"\"timestamp\"","metadata":"{}"},{"name":"artist","type":"\"string\"","metadata":"{}"},{"name":"title","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>datetime</th><th>artist</th><th>title</th></tr></thead><tbody><tr><td>2019-12-31T00:00:00.000+0000</td><td>Peja/slums Attack</td><td>Szacunek Ludzi Ulicy (Explicit)</td></tr><tr><td>2019-12-31T00:05:00.000+0000</td><td>Tymek/tede</td><td>Rainman (Explicit)</td></tr><tr><td>2019-12-31T00:08:00.000+0000</td><td>Taconafide</td><td>Metallica 808 (Explicit)</td></tr><tr><td>2019-12-31T00:12:00.000+0000</td><td>Nautilus</td><td>Blat</td></tr><tr><td>2019-12-31T00:16:00.000+0000</td><td>Young Multi</td><td>Jeden Dzien (Explicit)</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Manipulate dataframe, add basic new columns\n**New columns**\n* **year** - year derived from the datetime time stamp of each row\n* **radio_name** - the name of the radio that is derived from the path of the file\n\n**Things to be noticed:**   \n* **select** - similar to sql select, selects the columns for further use\n* **input_file_name()** - is the special function that is returning the filepath of the file from the particular row is coming\n* **withColumn** - function withColumn can be used to add a new column to existing dataframe\n* **attribute names** - attributes can be used in many different ways, here we have a name of the dataframe and the column name like dataframe[\"columnname\"] when the column exists in the original dataframe and only the \"columnname\" if the column has been derived. There are more options tough\n* **replace dataframe** - please also note that the dataframe is assigned back to the same name and will be \"overwriten\"\n* **list of avalaible functions** - list of all avaliable functions can be found here: https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html\n* **lazy evaluation aka transformations vs actions** - in databricks, or spark to be precise all transformations are lazy evaluation transformation which means that they won't be actually executed unless we provide an action, thefore the below command will be \"executed\" in just a second, but the .Count or some other functions that are providing the actions are taking longer and only them are applying and executing the actual code; please also note that spark will also try to optimize all of your code thanks to the lazy evaluations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d353c2e-e907-499c-bca9-e16c80a7f567"}}},{"cell_type":"code","source":["from  pyspark.sql.functions import input_file_name\nfrom pyspark.sql.functions import lit, split, reverse, regexp_replace, count, concat_ws\nfrom pyspark.sql.functions import year, date_format, hour, to_date\n\n# all functions -> https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html\n\ndf_playlist = df_playlist.select(df_playlist[\"artist\"], df_playlist[\"datetime\"], df_playlist[\"title\"]) \\\n                         .withColumn(\"year\", year(df_playlist[\"datetime\"])) \\\n                         .withColumn(\"radio_name\", split(regexp_replace(reverse(split(input_file_name(), \"/\"))[0], \".json\", \"\"), \"_\")[0]) \\\n                         .select( \\\n                            \"radio_name\", \\\n                            \"year\", \\\n                            df_playlist[\"datetime\"], \\\n                            df_playlist[\"artist\"], df_playlist[\"title\"] \\\n                          )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a82abd31-5186-4c7b-90ee-e0d84520611a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Display 5 sample rows"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9cb51327-c699-4582-aba1-08d19f99e6c7"}}},{"cell_type":"code","source":["display(\n  df_playlist.head(5)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0c14347-32c2-4177-a1b1-479956631ff7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Eska",2019,"2019-12-31T00:00:00.000+0000","Peja/slums Attack","Szacunek Ludzi Ulicy (Explicit)"],["Eska",2019,"2019-12-31T00:05:00.000+0000","Tymek/tede","Rainman (Explicit)"],["Eska",2019,"2019-12-31T00:08:00.000+0000","Taconafide","Metallica 808 (Explicit)"],["Eska",2019,"2019-12-31T00:12:00.000+0000","Nautilus","Blat"],["Eska",2019,"2019-12-31T00:16:00.000+0000","Young Multi","Jeden Dzien (Explicit)"],["Eska",2019,"2019-12-31T00:19:00.000+0000","Kubi Producent/otsoch/schafter/planbe","9 Zyc (Explicit)"],["Eska",2019,"2019-12-31T00:23:00.000+0000","Grubson","Naprawimy To (Explicit)"],["Eska",2019,"2019-12-31T00:28:00.000+0000","Mata","Patointeligencja (Explicit)"],["Eska",2019,"2019-12-31T00:32:00.000+0000","White 2115","Palac (Explicit)"],["Eska",2019,"2019-12-31T00:34:00.000+0000","Keke","Wyjebane Tak Mocno (Explicit)"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"radio_name","type":"\"string\"","metadata":"{}"},{"name":"year","type":"\"long\"","metadata":"{}"},{"name":"datetime","type":"\"timestamp\"","metadata":"{}"},{"name":"artist","type":"\"string\"","metadata":"{}"},{"name":"title","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>radio_name</th><th>year</th><th>datetime</th><th>artist</th><th>title</th></tr></thead><tbody><tr><td>Eska</td><td>2019</td><td>2019-12-31T00:00:00.000+0000</td><td>Peja/slums Attack</td><td>Szacunek Ludzi Ulicy (Explicit)</td></tr><tr><td>Eska</td><td>2019</td><td>2019-12-31T00:05:00.000+0000</td><td>Tymek/tede</td><td>Rainman (Explicit)</td></tr><tr><td>Eska</td><td>2019</td><td>2019-12-31T00:08:00.000+0000</td><td>Taconafide</td><td>Metallica 808 (Explicit)</td></tr><tr><td>Eska</td><td>2019</td><td>2019-12-31T00:12:00.000+0000</td><td>Nautilus</td><td>Blat</td></tr><tr><td>Eska</td><td>2019</td><td>2019-12-31T00:16:00.000+0000</td><td>Young Multi</td><td>Jeden Dzien (Explicit)</td></tr><tr><td>Eska</td><td>2019</td><td>2019-12-31T00:19:00.000+0000</td><td>Kubi Producent/otsoch/schafter/planbe</td><td>9 Zyc (Explicit)</td></tr><tr><td>Eska</td><td>2019</td><td>2019-12-31T00:23:00.000+0000</td><td>Grubson</td><td>Naprawimy To (Explicit)</td></tr><tr><td>Eska</td><td>2019</td><td>2019-12-31T00:28:00.000+0000</td><td>Mata</td><td>Patointeligencja (Explicit)</td></tr><tr><td>Eska</td><td>2019</td><td>2019-12-31T00:32:00.000+0000</td><td>White 2115</td><td>Palac (Explicit)</td></tr><tr><td>Eska</td><td>2019</td><td>2019-12-31T00:34:00.000+0000</td><td>Keke</td><td>Wyjebane Tak Mocno (Explicit)</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Manipulate dataframe, add basic new columns\n\n**New dataframe**\n* **df_stats** - dataframe with statistics about number of rows for each radio\n\n**New columns**\n* **cnt** - number of rows for each group, here for each radio\n\n**Things to be noticed:**   \n* **lit(x)** - lit function can be used to specify the constant value for all columns, please note that providing the value itself without that function will raise an exception\n* **goupby** - equvalent of sql groupby used to group the data by one or many columns\n* **agg** - function that can be used in combination with other aggregation functions, please note that the count is a different function\n* **alias** - with alias we can rename a column"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3ed08da-f3f0-4036-b0c2-bf331f0ab0a1"}}},{"cell_type":"code","source":["df_stats = df_playlist.select(\"radio_name\", \"datetime\", \"title\", \"artist\") \\\n               .groupBy(\"radio_name\") \\\n               .agg(count(lit(1)).alias(\"cnt\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fae4c4fa-77b8-4cb7-ac73-b5d529ff9475"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Visualizations in databricks notebook\n\n**Things to be noticed:**   \n* **display** - display is the magic Databricks function that can be used for visualization of many different objects including spark or pandas dataframes\n* **visualizations** - those are the built in visualization tools inside Databricks notebook, we don't use additional python packages here, please note that you can change the layout of the chart using the creator below the chart or grid, please also note that by default the display() function will show the grid with the data, but we can change to the charts"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8910288-75b0-45c1-acba-d8add9815d94"}}},{"cell_type":"code","source":["display(\n  df_stats\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d323a7e3-e44e-4046-ae69-f6142a365a88"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Eska",1360400],["Antyradio",991631],["RMFFM",1180904],["ZET",1081730]],"plotOptions":{"displayType":"plotlyBar","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"radio_name","type":"\"string\"","metadata":"{}"},{"name":"cnt","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>radio_name</th><th>cnt</th></tr></thead><tbody><tr><td>Eska</td><td>1360400</td></tr><tr><td>Antyradio</td><td>991631</td></tr><tr><td>RMFFM</td><td>1180904</td></tr><tr><td>ZET</td><td>1081730</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Export new dataframe to the - curated - layer\n\n**Things to be noticed:**   \n* **write** - save the data into the destination, please note that later on we use the .parquet() function to specify the format of output file\n* **partitionBy()** - partitionBy will partition the dataframe when saving in the data lake, that means that databricks will automatically save the dataframe to multiple directories acording to the columns specified in partitionBy function, here we will split the data into /radio_name=.../year=... folder structure, please also note that the columns from partitionBy function will be removed from the target file"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93a44621-579f-4713-a4a1-21e5a1bd0371"}}},{"cell_type":"code","source":["output_directory = \"abfss://learnsthnew@sdsalearnsthnew.dfs.core.windows.net/curated-initial/\"\ndf_playlist.write.mode('overwrite') \\\n                 .partitionBy(\"radio_name\", \"year\") \\\n                 .parquet(output_directory)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f43d5332-0f8c-448e-8c69-154900eaf3d0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Finish notebook execution and report success\n\n**Things to be noticed:**   \n* **dbutils.notebook.exit** - that magic command can be used to return a value from the notebook, can be very useful when there is a job that executes many notebooks or one notebook is executing another notebook"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fbe6059-aeb2-4627-adbb-695bf03de676"}}},{"cell_type":"code","source":["dbutils.notebook.exit(\"success\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb7fc7fb-6da7-4093-b8c2-8a2a93335ea9"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"learn-sth-new-elt-1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2448923008180566}},"nbformat":4,"nbformat_minor":0}
