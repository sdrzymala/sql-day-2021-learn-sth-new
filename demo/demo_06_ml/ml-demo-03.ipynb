{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to Azure ML workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\r\n",
        "from azureml.core import Workspace, Datastore, Dataset\r\n",
        "\r\n",
        "workspace_name = 'sd-ml'\r\n",
        "subscription_id = 'c374c749-c070-4b3b-9fb4-40a657b1d4a5' # subscription id of ADLS account\r\n",
        "resource_group = 'rs-sd-learn-sth-new' # resource group of ADLS account\r\n",
        "\r\n",
        "workspace = Workspace.get(\r\n",
        "    name = workspace_name,\r\n",
        "    subscription_id = subscription_id,\r\n",
        "    resource_group = resource_group\r\n",
        ")\r\n",
        "\r\n",
        "print (\"workspace to be used: \" + workspace.name)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "workspace to be used: sd-ml\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1620586375450
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get or set up datastore"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "datastore_name = 'learnsthnew_datastore'\r\n",
        "filesystem = 'learnsthnew'\r\n",
        "\r\n",
        "subscription_id = 'c374c749-c070-4b3b-9fb4-40a657b1d4a5' # subscription id of ADLS account\r\n",
        "resource_group = 'rs-sd-learn-sth-new' # resource group of ADLS account\r\n",
        "\r\n",
        "account_name = 'sdsalearnsthnew' # ADLS Gen2 account name\r\n",
        "tenant_id = '680b5d20-b41e-46f8-a077-f482d0c64dbb' # tenant id of service principal\r\n",
        "client_id = '6df9c689-4854-45ec-a9c2-55194c54c511' # client id of service principal\r\n",
        "client_secret = 'Is5A5Jctw1~Ge-hi4EOS_RmRahG5_s43F4' # the secret of service principal\r\n",
        "\r\n",
        "try:\r\n",
        "    datastore = Datastore.get(\r\n",
        "        workspace = workspace,\r\n",
        "        datastore_name = datastore_name\r\n",
        "    )\r\n",
        "except Exception as exc:\r\n",
        "    datastore = Datastore.register_azure_data_lake_gen2(\r\n",
        "        workspace = workspace,\r\n",
        "        subscription_id = subscription_id,\r\n",
        "        resource_group = resource_group,\r\n",
        "        datastore_name = datastore_name,\r\n",
        "        account_name = account_name, # ADLS Gen2 account name\r\n",
        "        filesystem = filesystem, # ADLS Gen2 filesystem\r\n",
        "        tenant_id = tenant_id, # tenant id of service principal\r\n",
        "        client_id = client_id, # client id of service principal\r\n",
        "        client_secret = client_secret # the secret of service principal\r\n",
        "    )\r\n",
        "\r\n",
        "print (\"datastore to be used: \" + datastore.name)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datastore to be used: learnsthnew_datastore\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620586377157
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get or set up dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Datastore, Dataset\r\n",
        "\r\n",
        "datastore_name = 'learnsthnew_datastore'\r\n",
        "dataset_name = 'playlist_statistics'\r\n",
        "dataset_description = 'radio song playlist statistics'\r\n",
        "\r\n",
        "try:\r\n",
        "    dataset = Dataset.get_by_name(\r\n",
        "        workspace = workspace, \r\n",
        "        name = dataset_name\r\n",
        "    )\r\n",
        "except Exception as exc:\r\n",
        "    datastore_paths = [(datastore, '/analytics/playlist_statistics.parquet/*.parquet')]\r\n",
        "    dataset = Dataset.Tabular.from_parquet_files(path=datastore_paths, validate=False)\r\n",
        "    dataset = dataset.register(\r\n",
        "        workspace=workspace,\r\n",
        "        name = dataset_name,\r\n",
        "        description = dataset_description\r\n",
        "   )\r\n",
        "\r\n",
        "print (\"dataset to be used: \" + dataset.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset to be used: playlist_statistics\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620586379352
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create prediction model using DecisionTreeClassifier as an experiment, validate model and register in Azure ML \r\n",
        "\r\n",
        "* **experiment = Experiment()** - we will create an Experiment using AzureML SDK, that will allow us to keep track of the model in Azure ML workspace\r\n",
        "* **run = experiment.start_logging()** - will allow us to log the script execution and save the logs and make them avaliable in Azure ML workspace\r\n",
        "* **AUC** - another metric that is used to validate the model performance\r\n",
        "* **run.register_model** - will allow us to register the model in the AzureML workspace\r\n",
        "* **run.upload_file** - is uploading the model content to the Azure ML workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "from azureml.core import Model\r\n",
        "from azureml.core import Run\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import joblib\r\n",
        "\r\n",
        "import logging\r\n",
        "logging.basicConfig(filename='./outputs/log.txt', \r\n",
        "                            filemode='a', \r\n",
        "                            format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s', \r\n",
        "                            datefmt='%H:%M:%S',\r\n",
        "                            level=logging.DEBUG)\r\n",
        "\r\n",
        "# Create an Azure ML experiment in your workspace\r\n",
        "experiment = Experiment(workspace=workspace, name=\"try-sth-new-songs\")\r\n",
        "\r\n",
        "run = experiment.start_logging()\r\n",
        "logging.info(\"Start experiment\")\r\n",
        "print(\"Starting experiment:\", experiment.name)\r\n",
        "\r\n",
        "\r\n",
        "print(\"Loading Data...\")\r\n",
        "df = dataset.to_pandas_dataframe()\r\n",
        "\r\n",
        "print(\"Separating features and labels...\")\r\n",
        "features = [\"radio_name\", \"month_name\", \"artist_and_title\"]\r\n",
        "label = 'played'\r\n",
        "X, y = df[features].values, df[label].values\r\n",
        "\r\n",
        "print(\"Spliting data 70%-30% into training set and test set...\")\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\r\n",
        "print ('Training cases: %d\\nTest cases: %d' % (X_train.shape[0], X_test.shape[0]))\r\n",
        "\r\n",
        "\r\n",
        "print (\"Defining preprocessing for categorical features (encode the Age column...\")\r\n",
        "categorical_features = [0,1]\r\n",
        "categorical_transformer = Pipeline(steps=[\r\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\r\n",
        "categorical_features_song = [2]\r\n",
        "categorical_transformer_song = Pipeline(steps=[\r\n",
        "    ('labelenc', OrdinalEncoder())])\r\n",
        "\r\n",
        "\r\n",
        "print (\"Combining preprocessing steps...\")\r\n",
        "preprocessor = ColumnTransformer(\r\n",
        "    transformers=[\r\n",
        "        ('cat', categorical_transformer, categorical_features),\r\n",
        "        ('catsong', categorical_transformer_song, categorical_features_song)\r\n",
        "    ])\r\n",
        "\r\n",
        "print (\"Creating preprocessing and training pipeline...\")\r\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\r\n",
        "                           ('logregressor', DecisionTreeClassifier())])\r\n",
        "\r\n",
        "print (\"Fitting the pipeline to train a DecisionTreeClassifier model on the training set...\")\r\n",
        "model = pipeline.fit(X_train, (y_train))\r\n",
        "\r\n",
        "print(\"Calculating AUC...\")\r\n",
        "y_scores = model.predict_proba(X_test)\r\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\r\n",
        "print('AUC: ' + str(auc))\r\n",
        "run.log('AUC', np.float(auc))\r\n",
        "\r\n",
        "print(\"Calculating Accuracy score...\")\r\n",
        "predictions = model.predict(X_test)\r\n",
        "accuracy_score = accuracy_score(y_test, predictions)\r\n",
        "print('Accuracy: ', accuracy_score)\r\n",
        "run.log('Accuracy', np.float(accuracy_score))\r\n",
        "\r\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\r\n",
        "conf_matrix_json = \"\"\"{\r\n",
        "       \"schema_type\": \"confusion_matrix\",\r\n",
        "       \"schema_version\": \"1.0.0\",\r\n",
        "       \"data\": {\r\n",
        "           \"class_labels\": [\"0\", \"1\"],\r\n",
        "           \"matrix\": [\r\n",
        "               [\"\"\"+str(conf_matrix[0][0])+\"\"\", \"\"\"+str(conf_matrix[0][1])+\"\"\"],\r\n",
        "               [\"\"\"+str(conf_matrix[1][0])+\"\"\", \"\"\"+str(conf_matrix[1][1])+\"\"\"]\r\n",
        "           ]\r\n",
        "       }\r\n",
        "   }\"\"\"\r\n",
        "run.log_confusion_matrix(name=\"confusion matrix\", value=conf_matrix_json)\r\n",
        "print (conf_matrix)\r\n",
        "\r\n",
        "print (\"Saving the model...\")\r\n",
        "model_file = 'song_prd_model.pkl'\r\n",
        "joblib.dump(value=model, filename=model_file)\r\n",
        "run.upload_file(name = 'models/' + model_file, path_or_stream = './' + model_file)\r\n",
        "\r\n",
        "print (\"Completing the run...\")\r\n",
        "logging.info(\"Complete run\")\r\n",
        "run.complete()\r\n",
        "\r\n",
        "# Register the model\r\n",
        "run.register_model(model_path='models/song_prd_model.pkl', model_name='song_prd_model',\r\n",
        "                   tags={'Training context':'Inline Training'},\r\n",
        "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\r\n",
        "\r\n",
        "print('Model trained and registered.')\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting experiment: try-sth-new-songs\n",
            "Loading Data...\n",
            "Separating features and labels...\n",
            "Spliting data 70%-30% into training set and test set...\n",
            "Training cases: 19237748\n",
            "Test cases: 8244750\n",
            "Defining preprocessing for categorical features (encode the Age column...\n",
            "Combining preprocessing steps...\n",
            "Creating preprocessing and training pipeline...\n",
            "Fitting the pipeline to train a DecisionTreeClassifier model on the training set...\n",
            "Calculating AUC...\n",
            "AUC: 0.9223391352372876\n",
            "Calculating Accuracy score...\n",
            "Accuracy:  0.9571007004457383\n",
            "[[7884085    5635]\n",
            " [ 348059    6971]]\n",
            "Saving the model...\n",
            "Completing the run...\n",
            "Model trained and registered.\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620594080737
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrive model from workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = workspace.models['song_prd_model']\r\n",
        "print(model.name, 'version', model.version)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "song_prd_model version 4\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620594416534
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create webservice to use the model using the API"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "\r\n",
        "folder_name = 'song_service'\r\n",
        "\r\n",
        "# Create a folder for the web service files\r\n",
        "experiment_folder = './' + folder_name\r\n",
        "os.makedirs(experiment_folder, exist_ok=True)\r\n",
        "\r\n",
        "print(folder_name, 'folder created.')\r\n",
        "\r\n",
        "# Set path for scoring script\r\n",
        "script_file = os.path.join(experiment_folder,\"predict_song.py\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "song_service folder created.\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620595568917
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create webservice script"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_file\r\n",
        "import json\r\n",
        "import joblib\r\n",
        "import numpy as np\r\n",
        "from azureml.core.model import Model\r\n",
        "\r\n",
        "# Called when the service is loaded\r\n",
        "def init():\r\n",
        "    global model\r\n",
        "    # Get the path to the deployed model file and load it\r\n",
        "    model_path = Model.get_model_path('song_prd_model')\r\n",
        "    model = joblib.load(model_path)\r\n",
        "\r\n",
        "# Called when a request is received\r\n",
        "def run(raw_data):\r\n",
        "    # Get the input data as a numpy array\r\n",
        "    data = np.array(json.loads(raw_data)['data'])\r\n",
        "    # Get a prediction from the model\r\n",
        "    predictions = model.predict(data)\r\n",
        "    # Get the corresponding classname for each prediction (0 or 1)\r\n",
        "    classnames = ['0', '1']\r\n",
        "    predicted_classes = []\r\n",
        "    for prediction in predictions:\r\n",
        "        predicted_classes.append(classnames[prediction])\r\n",
        "    # Return the predictions as JSON\r\n",
        "    return json.dumps(predicted_classes)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./song_service/predict_song.py\n"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Webservice will be running from container and we need to create a file with the dependencies to be installed"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.conda_dependencies import CondaDependencies \r\n",
        "\r\n",
        "# Add the dependencies for our model (AzureML defaults is already included)\r\n",
        "myenv = CondaDependencies()\r\n",
        "myenv.add_conda_package('scikit-learn')\r\n",
        "\r\n",
        "# Save the environment config as a .yml file\r\n",
        "env_file = os.path.join(experiment_folder,\"song_env.yml\")\r\n",
        "with open(env_file,\"w\") as f:\r\n",
        "    f.write(myenv.serialize_to_string())\r\n",
        "print(\"Saved dependency info in\", env_file)\r\n",
        "\r\n",
        "# Print the .yml file\r\n",
        "with open(env_file,\"r\") as f:\r\n",
        "    print(f.read())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved dependency info in ./song_service/song_env.yml\n",
            "# Conda environment specification. The dependencies defined in this file will\n",
            "# be automatically provisioned for runs with userManagedDependencies=False.\n",
            "\n",
            "# Details about the Conda environment file format:\n",
            "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n",
            "\n",
            "name: project_environment\n",
            "dependencies:\n",
            "  # The python interpreter version.\n",
            "  # Currently Azure ML only supports 3.5.2 and later.\n",
            "- python=3.6.2\n",
            "\n",
            "- pip:\n",
            "    # Required packages for AzureML execution, history, and data preparation.\n",
            "  - azureml-defaults\n",
            "\n",
            "- scikit-learn\n",
            "channels:\n",
            "- anaconda\n",
            "- conda-forge\n",
            "\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620595569145
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy model and create the webservice"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.webservice import AciWebservice\r\n",
        "from azureml.core.model import InferenceConfig\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Configure the scoring environment\r\n",
        "inference_config = InferenceConfig(runtime= \"python\",\r\n",
        "                                   entry_script=script_file,\r\n",
        "                                   conda_file=env_file)\r\n",
        "\r\n",
        "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\r\n",
        "\r\n",
        "service_name = \"song-service\"\r\n",
        "\r\n",
        "service = Model.deploy(workspace, service_name, [model], inference_config, deployment_config,overwrite=True)\r\n",
        "\r\n",
        "service.wait_for_deployment(True)\r\n",
        "print(service.state)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
            "Running\n",
            "2021-05-09 21:26:12+00:00 Creating Container Registry if not exists.\n",
            "2021-05-09 21:26:13+00:00 Registering the environment.\n",
            "2021-05-09 21:26:14+00:00 Use the existing image.\n",
            "2021-05-09 21:26:14+00:00 Generating deployment configuration.\n",
            "2021-05-09 21:26:15+00:00 Submitting deployment to compute..\n",
            "2021-05-09 21:26:21+00:00 Checking the status of deployment song-service..\n",
            "2021-05-09 21:37:24+00:00 Checking the status of inference endpoint song-service.\n",
            "Succeeded\n",
            "ACI service creation operation finished, operation \"Succeeded\"\n",
            "Healthy\n"
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620596245693
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrive the webservice endpoint URL"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = service.scoring_uri\r\n",
        "print(endpoint)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://18a690c1-d1e2-404a-9cf6-32ac6bd76b8e.westeurope.azurecontainer.io/score\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620596245809
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use the webservice to predict the value"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\r\n",
        "import json\r\n",
        "\r\n",
        "x_new = [\r\n",
        "    [\"RMFFM\", \"December\", \"Wham! - Last Christmas\"],\r\n",
        "    [\"RMFFM\", \"June\"    , \"Wham! - Last Christmas\"],\r\n",
        "    [\"RMFFM\", \"December\", \"Rotary - Na Jednej Z Dzikich Plaż\"],\r\n",
        "    [\"RMFFM\", \"June\", \"Rotary - Na Jednej Z Dzikich Plaż\"]\r\n",
        "]\r\n",
        "\r\n",
        "# Convert the array to a serializable list in a JSON document\r\n",
        "input_json = json.dumps({\"data\": x_new})\r\n",
        "\r\n",
        "# Set the content type\r\n",
        "headers = { 'Content-Type':'application/json' }\r\n",
        "\r\n",
        "predictions = requests.post(endpoint, input_json, headers = headers)\r\n",
        "predicted_classes = json.loads(predictions.json())\r\n",
        "\r\n",
        "for i in range(len(x_new)):\r\n",
        "    print (\"The song in radio in given month: {}\".format(x_new[i]), predicted_classes[i] )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The song in radio in given month: ['RMFFM', 'December', 'Wham! - Last Christmas'] 1\n",
            "The song in radio in given month: ['RMFFM', 'June', 'Wham! - Last Christmas'] 0\n",
            "The song in radio in given month: ['RMFFM', 'December', 'Rotary - Na Jednej Z Dzikich Plaż'] 0\n",
            "The song in radio in given month: ['RMFFM', 'June', 'Rotary - Na Jednej Z Dzikich Plaż'] 1\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620596245901
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}